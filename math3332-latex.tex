\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath} 
\usepackage{graphicx} 
\usepackage{amssymb}
\usepackage{emoji}

\title{MATH3332 Data Analytic Tools}
\author{Ye Moe}
\date{HKUST Fall 2022}

\begin{document}

\maketitle

\section*{Introduction}


The purpose of this course is to introduce some crucial mathematical analysis tools for data analysis/machine learning. 



According to \textit{Pedro Domingos},

\begin{center}
    $Learning = Representation + Evaluation + Optimization$
\end{center}


\begin{enumerate}
    \item Representation
    \begin{itemize}
        \item How do we represent a learner? Which set should a learner be in? This set is called the hypothesis space of the learner. Some related tools are "space of functions".
        \item How do we represent the input? Potential tools include vectors, graphs, manifolds, \dots
    \end{itemize}
    
    \item Evaluation 
    \begin{itemize}
        \item How to pick the best learner from the hypothesis space? Needs calculus of "functions of functions" also known as functionals.
        \item How to represent the input effectively? Needs Linear Algebra, Graph Theory, Manifolds Calculus, Harmonic Analysis, \dots
        
    \end{itemize}
    
    \item Optimization
    \begin{itemize}
        \item Numerical optimization solver - how to get the optimal solution numerically by a computer? Many of the resulting optimization is convex optimization and it is related to Convex Analysis.
    \end{itemize}
\end{enumerate}

So this course consists of some
\begin{itemize}
        \item Basic functional analysis (calculus of functionals)
        \item Basic convex analysis 
        \item Fourier analysis and Wavelet analysis (if time allowed)
\end{itemize}

\pagebreak

\section*{Normed and Inner Product Space}
\subsection*{2.1 Vector Spaces}
    \textbf{Definition:} A vector space over $\mathbb{R}$ is a set $\mathbb{V}$ together with two functions. 
    \begin{enumerate}
        \item Vector addition: $+ : (\mathbb{V},\mathbb{V}) \to \mathbb{V}$ \\i.e. $\forall x, y \in \mathbb{V}, x + y \in \mathbb{V}$
        \item Scalar multiplication: $. : (\mathbb{R},\mathbb{V}) \to \mathbb{V}$ 
        \\i.e. $\forall \alpha \in  \mathbb{R}, x \in \mathbb{V}, \alpha x \in \mathbb{V}$ 
    \end{enumerate}
    
These two functions should satisfy the following eight properties:
\begin{enumerate}
    \item Associativity of addition: $ x + (y + z) = (x + y) + z, \forall x, y, z \in \mathbb{V}$
    \item Commutativity of addition: $ x + y = y + x,  \forall x, y \in \mathbb{V}$
    \item Zero vector: $\exists$ an element, denoted by 0 in $\mathbb{V}$ s.t.  $x + 0 = 0 + x = x, \forall x \in \mathbb{V}$
    \item Negative vector: $\forall x \in \mathbb{V}, \exists$ an elements, denoted by $-x \in \mathbb{V}$ s.t. $x + (-x) = (-x) + x = 0 $
    \item $\forall x \in \mathbb{V}, 1\cdot x = x$
    \item $\forall x \in \mathbb{V}, \alpha, \beta \in \mathbb{R}, \alpha(\beta x) = (\alpha \beta)x$
    \item $\forall x \in \mathbb{V}$ and $\alpha,\beta \in \mathbb{R}, (\alpha + \beta)x = \alpha x + \beta x$
    \item $\forall x, y \in \mathbb{V}, \alpha(x + y) = \alpha x + \alpha y$
    
\end{enumerate} 
    
\textbf{Remarks:} We can define vector space over the complex domain $\mathbb{C}$, but since vector space over complex domain $\mathbb{C}$ is used very rarely, we will only consider vector space in the real domain $\mathbb{R}$. \\

Some examples of vector space include $\mathbb{R}$, $\mathbb{R}^{n}$, $\mathbb{R} ^{m \times n}$, $\mathbb{R} ^ {m \times n \times l}$, $C[a,b]$ and $L_\infty$.\\

\includegraphics[width=5cm, height=4cm]{Meme.jpeg}
\pagebreak

\textbf{Example:} Prove that $\mathbb{R}^{n}$ is a vector space.\\
    $\forall x, y \in \mathbb{R}^{n}$ and $\alpha \in \mathbb{R}$,
    \begin{center}
        \item $ x + y = $ \begin{bmatrix} x_{1} \\x_{2} \\\vdots \\ x_{n}\end{bmatrix} + \begin{bmatrix} y_{1} \\y_{2} \\\vdots \\ y_{n}\end{bmatrix} = \begin{bmatrix} x_{1} + y_{1} \\x_{2} + y_{2}\\\vdots \\ x_{n} + y_{n} \end{bmatrix} \in \mathbb{R}^{n} \\
        \item $\alpha x = \alpha$ \begin{bmatrix} x_{1} \\x_{2} \\\vdots \\ x_{n}\end{bmatrix} = \begin{bmatrix} \alpha x_{1} \\ \alpha x_{2} \\\vdots \\ \alpha x_{n}\end{bmatrix} \in \mathbb{R}^{n} \\
    \end{center}
Since it is closed under both vector addition and scalar multiplication, $\mathbb{R} ^ {n}$ is a vector space. \\


\textbf{Example:} Prove that $C[a,b]$ is a vector space. \\
    $\forall f, g \in C[a,b]$ and $\alpha \in \mathbb{R}$,
    \begin{center}
        \item $ f(t) + g(t) = (f + g)(t) \in C[a,b]$, $\forall t \in [a,b]$ 
        \item $ \alpha f(t) = (\alpha f)(t) \in C[a,b]$, $\forall t \in [a,b]$
        
    \end{center} 
Since it is closed under both vector addition and scalar multiplication, $C[a,b]$ is a vector space. \\

\textbf{Remarks:} $C[a,b]$ is referred to as a function space, since any vector in this vector space is a function. It might be a hypothesis space of a learner with one input and one output, i.e. Find a $f \in C[a,b]$ s.t. $f(x_i) \approx f(y_i)$ for all $i$. \\

\pagebreak

\textbf{Example:} Prove that $L_\infty$ is a vector space.\\
    \begin{center}
        $L_\infty$ = \{\begin{bmatrix} a_{1} \\a_{2} \\\vdots \end{bmatrix} \mid \exists \text{ a finite number $c$ s.t.} $\mid{a_i} \mid \leq c$ for any i\} \\ 
    \end{center}
    $\forall a, b \in L_\infty $ and $\alpha \in \mathbb{R}$,
    \begin{center}
        \item $a + b = $ \begin{bmatrix} a_{1} \\a_{2} \\\vdots \end{bmatrix} + \begin{bmatrix} b_{1} \\b_{2} \\\vdots \end{bmatrix} = \begin{bmatrix} a_{1} + b_{1} \\ a_{2} + b_{2} \\\vdots \end{bmatrix} \in L_\infty \\ 
        \item $\alpha a = \alpha$ \begin{bmatrix} a_{1} \\a_{2} \\\vdots \end{bmatrix} = \begin{bmatrix} \alpha a_{1} \\ \alpha a_{2} \\\vdots \end{bmatrix} \in L_\infty \\
    \end{center} 
Since it is closed under both vector addition and scalar multiplication, $L_\infty$ is a vector space. \\

\textbf{Remarks:} This vector space can be used to model stock prices with a very fine time resolution.\\


\textbf{Example:} Consider the set of all strings. \\
    \begin{center}
        $ \text{I' + 'am'} \neq \text{'am' + 'I'}$ 
    \end{center}
The set of all strings violates the commutative properties of a vector space, therefore it isn't a vector space. Hence, we cannot use vector space to model text data in this na√Øve way. \\

How do we "vectorize" the text data? \\ 
This is a fundamental question in text data analysis.


\pagebreak
\subsection*{2.2 Normed and Banach Space}
In order to do calculus on vector spaces, we need to define 'distance/closeness' between vectors. \\

\raggedright Let $\mathbb{V}$ be a vector space. Let $x, y \in \mathbb{V}$. Then, \\ 
    \begin{center}
        $distance(x,y) = distance(x - y, y - y) = distance (x - y, 0) = \text{length of x - y}$ 
    \end{center} \\

\raggedright \textbf{Remarks:} Distance should be shift invariant. \\ 
    
\raggedright To define distance, we only need to define the length of vectors.
Let x \in \mathbb{V}. \\ Denote $\Vert x \Vert$ be the length of x. Then $\Vert x \Vert$ should satisfy: 
\begin{enumerate}
    \item $\Vert x \Vert \geq 0\text{ (the length should be non-negative)}$ \\
    Moreover, $\Vert x \Vert = 0 \iff x = 0 \text{(only zero vector has a zero length)}$
    \item $\Vert \alpha x \Vert = \mid{\alpha} \mid \Vert x \Vert \\ \text{(length of a scaling of a vector is a scaling of the length of the vector)}$ \\
    \begin{center}
        \fbox{\includegraphics[width=6cm, height=5cm]{1.jpg}}
    \end{center} 
    \item $\Vert x + y \Vert \leq $\Vert x \Vert$ + $\Vert y \Vert \text{ (also known as triangle inequality)}$ \\ (length of direct path should be smaller than the length of indirect path) \\
    \begin{center}
        \fbox{\includegraphics[width=8cm, height=5cm]{2.jpg}}
    \end{center}
\end{enumerate}
\pagebreak

\textbf{Definition: } Let $\mathbb{V}$ be a vector space. A norm on $\mathbb{V}$ is a function $\Vert \cdot \Vert: \mathbb{V} \to \mathbb{R}$ such that:
\begin{enumerate}
    \item  $\Vert x \Vert \geq 0 \ \forall x \in \mathbb{V}$ and $\Vert x \Vert =  0 \iff x = 0$
    \item $\Vert \alpha x \Vert = \mid{\alpha} \mid \Vert x \Vert, \ \forall \alpha \in \mathbb{R}, x \in \mathbb{V}$
    \item $\Vert x + y \Vert \leq \Vert x \Vert + \Vert y \Vert, \ $\forall x, y \in \mathbb{V}$
\end{enumerate}
\textbf{Example: } $\mathbb{R}$ is a vector space over $\mathbb{R}$. \\
Let $\Vert x \Vert = \mid{x}\mid$ $\forall x \in \mathbb{R}$. Then it is a norm on $\mathbb{R}$. 

\bigbreak

\textbf{Example: } $\mathbb{R}^{n}$ is a vector space over $\mathbb{R}$. \\
There are many norms on $\mathbb{R}^{n}$. 
\begin{itemize}
    \item 2-norm: (Euclidean Norm) \\
        $\Vert x \Vert _2 =(\sum_{i=1}^{n} x_i ^ 2)^\frac{1}{2}$ \\
        \bigbreak
        \textbf{Question: } Prove that $\Vert \cdot \Vert _2$ is indeed a norm for $\mathbb{R}^{n}$. \\
        $\forall x, y \in \mathbb{R}^{n}$ and $\alpha \in \mathbb{R}$,
        \begin{center}
            \item $\Vert x \Vert _2 = (\sum_{i=1}^{n} x_i ^ 2)^\frac{1}{2} \geq 0 \bigbreak \Vert x \Vert _2 = 0 \iff \sum_{i=1}^{n} x_i ^ 2 = 0 \iff x_i ^2 = 0, \ i = 1, ... ,n \bigbreak \iff x_i = 0, \ i = 1, ... ,n \iff x = 0$ \bigbreak
            \item $\Vert \alpha x \Vert _2 = (\sum_{i=1}^{n} (\alpha x_i) ^ 2)^\frac{1}{2} = (\alpha ^ 2 \sum_{i=1}^{n} x_i ^ 2)^\frac{1}{2}
            = \mid{\alpha}\mid (\sum_{i=1}^{n} x_i ^ 2)^\frac{1}{2} = \mid{\alpha}\mid \Vert x \Vert _2$  
            \bigbreak
            \item $\Vert x + y \Vert _2 ^ 2 = \Vert x \Vert _2 ^ 2 + \Vert y  \Vert _2 ^ 2 + 2 \langle x,y\rangle  \bigbreak \leq \Vert x \Vert _2 ^ 2 + \Vert y  \Vert _2 ^ 2 + 2  \Vert x \Vert _2 \Vert y \Vert _2 \text{ (By Cauchy-Schwartz inequality)} \bigbreak = (\Vert x \Vert _2 + \Vert y \Vert _2)^2 \bigbreak \Vert x + y \Vert _2 \leq \Vert x \Vert _2 + \Vert y \Vert _2$ 
            \bigbreak
        \end{center}
    \item 1-norm: \\
        $\Vert x \Vert _1 =\sum_{i=1}^{n} \mid{x_i}\mid$
    \item $\infty$-norm: \\
        $\Vert x \Vert _\infty = \underset{1 \leq i \leq n}{\max} \mid{x_i}\mid$
    \item p-norm: \\
        $\Vert x \Vert _p =(\sum_{i=1}^{n} \mid{x_i}\mid ^ p)^\frac{1}{p}$ 

\end{itemize}
\textbf{Fact: }
$\Vert x \Vert _p$ is a norm on $\mathbb{R}^{n}$ \iff \ $p \geq 1$
\pagebreak

\textbf{Geometric definition of different norms in $\mathbb{R}^{n}$} \\ 

\bigbreak
\begin{center}
\fbox{\includegraphics[width=10cm, height=7cm]{3.jpg}}
\end{center} 

Note that $(\mathbb{R}^{n},\Vert \cdot \Vert _1), (\mathbb{R}^{n},\Vert \cdot \Vert _2), (\mathbb{R}^{n},\Vert \cdot \Vert _\infty), \dots$ are all different normed spaces. So for a given vector space, we can obtain various normed space by choosing different norms. Also, $\Vert x \Vert _p \leq \Vert x \Vert _q$ if $p \geq q$.

\bigbreak
\textbf{Example: } $\mathbb{R}^{m \times n}$ is a vector space over $\mathbb{R}$.
\begin{enumerate}
    \item  $\mathbb{R}^{m \times n}$ can be viewed as  $\mathbb{R}^{mn}$. \\
        \begin{center}
            \fbox{\includegraphics[width=6cm, height=4cm]{4.jpeg}}
        \end{center}
        We can define vector p-norm for  $\mathbb{R}^{m \times n}$.
        \begin{itemize}
            \item p = 1 \\
            $\Vert A \Vert_{1,vec} = \sum_{i=1}^{m} \sum_{j=1}^{n} \mid{a_{ij}}\mid$ 
            \pagebreak
            \item p = 2 \\
            $\Vert A \Vert_{2,vec} = (\sum_{i=1}^{m} \sum_{j=1}^{n} \mid{a_{ij}} \mid ^ 2 ) ^ \frac{1}{2}$ \\
            \bigbreak
            This norm is widely known as the Frobenius norm denoted as 
            $\Vert A \Vert_{F}$. 
            \item p = $\infty$\\
            $\Vert A \Vert_{\infty,vec} = \underset{i=1,...,m}{\max} \ \underset{j=1,...,n}{\max} \mid{a_{ij}} \mid$
        \end{itemize}
    \item $\mathbb{R}^{m \times n}$ can be viewed as linear transformation from $\mathbb{R}^{n} \to \mathbb{R}^{m}$. \\
        We can define matrix p-norm for $\mathbb{R}^{m \times n}$. \\
        \begin{center}
            $\Vert A \Vert_p = \underset{x \neq 0, x \in \mathbb{R}^{n}}{\max} \frac{\Vert Ax \Vert_p}{\Vert x \Vert_p} = \underset{\Vert x \Vert_p = 1}{\max} \Vert Ax \Vert_p$  
        \end{center}
        \begin{itemize}
            \item p = 1 \\
            $\Vert A \Vert_1 = \underset{1 \leq j \leq n}{\max} \sum_{i=1}^{m} \mid{a_{ij}\mid} = \text{maximum absolute column sum}$
            \item p = $\infty$ \\
            $\Vert A \Vert_\infty = \underset{1 \leq i \leq m}{\max} \sum_{j=1}^{n} \mid{a_{ij}\mid} = \text{maximum absolute row sum}$
            \item p = 2 \\
            $\Vert A \Vert_2 = \text{maximum singular value of A}$
        \end{itemize}
    \item We can also define other matrix norms. \\
        \begin{enumerate}
            \item We can use different norms in $\mathbb{R}^{n}$ and $\mathbb{R}^{m}$. 
                    \begin{center}
                        $\Vert A \Vert_{p \to q} = \underset{\Vert x \Vert_p = 1}{\max} \Vert Ax \Vert_q$
                    \end{center}
            \item The nuclear norm $\Vert \cdot \Vert_*$
        \end{enumerate}
\end{enumerate}

\textbf{Example: } $C[a,b]$ is a vector space over $\mathbb{R}$. \\
$\forall f \in C[a,b]$, define
    \begin{center}
        $\Vert f \Vert_\infty = \underset{t \in [a,b]}{\sup} \mid{f(t)}\mid$
    \end{center}
    We can check that $\Vert \cdot \Vert_\infty$ is indeed a norm on $C[a,b]$. \\
    The distance of two function $f,g \in C[a,b]$ is given by
    \begin{center}
        $\Vert f - g \Vert_\infty = \underset{t \in [a,b]}{\sup} \mid{f(t) - g(t)}\mid$ \\
        \fbox{\includegraphics[width=6cm, height=4cm]{5.jpeg}}
    \end{center}

\pagebreak
Some other norms on $C[a,b]$. 
\begin{enumerate}
    \item $\Vert f \Vert_1 = \int_{b}^{a} \mid{f(t)}\mid dt}$
    \item $\Vert f \Vert_2= (\int_{b}^{a} \mid{f(t)}\mid ^ 2 dt)^ \frac{1}{2}}$
    \item $\Vert f \Vert_p= (\int_{b}^{a} \mid{f(t)}\mid ^ p dt)^ \frac{1}{p}}$
\end{enumerate}
    
\textbf{Example: } $L_\infty = \{a | a \text{ is a infinite sequence and } \exists \ c >  0 \text{ s.t.} \mid{a_i}\mid \ \leq \ c, \forall i\}$

\begin{enumerate}
    \item $\forall a \in L_\infty$, define
        \begin{center}
            $\Vert a \Vert_\infty = \underset{i}{\sup} \mid{a_i}\mid$
        \end{center}
        \textbf{Remarks:} You cannot replace $\sup$ here with $\max$.
    \item Define $\Vert a \Vert_p = (\sum_{i=1}^{\infty} \mid{a_i}\mid ^ p)^\frac{1}{p} \ \forall a \in L_\infty$ but this is not a norm on $L_\infty$. \\
    
    e.g. $a =\begin{bmatrix} 1 \\ \frac{1}{2} \\ \vdots  \\ \frac{1}{i} \\ \vdots \end{bmatrix} \in L_\infty, \text{ but } \Vert a \Vert_1 = \sum_{i=1}^{\infty} \mid{a_i}\mid = \sum_{i=1}^{\infty} \frac{1}{i} = \infty$ \\
    
    So, $\Vert \cdot \Vert_1$ is not a norm on $L_\infty$. \\
    Instead, we consider \\
        \begin{center}
            $L_p = \{a \in L_\infty | \Vert a \Vert_p <  \infty\} \subset L_\infty$ \\
            $\Vert \cdot \Vert_p$ is a norm on $L_p$.
        \end{center}
    e.g. $a =\begin{bmatrix} 1 \\ \frac{1}{2} \\ \vdots  \\ \frac{1}{i} \\ \vdots \end{bmatrix} \in L_\infty$ \\
    $\Vert a \Vert_\infty = 1 \text{, } \Vert a \Vert_2 = (\sum_{i=1}^{\infty} \frac{1}{i^2}) ^ \frac{1}{2} = (\frac{\pi^2}{6}) ^ \frac{1}{2} = \frac{\pi}{\sqrt{6}} \text{,} \Vert a \Vert_1 = \infty$ \\
    So, $a \in L_\infty$, $a \in L_2$ but $a \notin L_1$. Indeed, $a \in L_p \ \forall p > 1$. 
        
\end{enumerate}

\pagebreak
\textbf{Limit and Convergence on Normed Vector Space} \\ 
To define calculus, we first need to define convergent sequence. \\
Let $\mathbb{V}$ be a normed vector space. Let $\{x^{(k)}\}_{k \in \mathbb{N}}$ be a sequence in $\mathbb{V}$, Let $ x \in \mathbb{V}$. We say $\{x^{(k)}\}_{k \in \mathbb{N}}$ converges to x, denoted by $x^{(k)} \to x$, if 
    \begin{center}
        $\underset{k \to \infty}{\lim} \Vert x^{(k)}- x \Vert = 0$ \\
        $\underset{k \to \infty}{\lim} \Vert x^{(k)} - x \Vert = 0 \iff x^{(k)} \to x$ 
    \end{center}

\bigbreak

\textbf{Example: } Consider $\mathbb{R}^{n}$ with $\Vert \cdot \Vert_2$, 
    \begin{center}
        Let $x^{(k)} = \begin{bmatrix} \frac{1}{k} \\ \frac{2}{k} \\ \vdots \\ \frac{n}{k} \end{bmatrix} \in \mathbb{R}^{n}$ and $x = \begin{bmatrix} 0 \\ 0 \\ \vdots \\ 0 \end{bmatrix} \in \mathbb{R}^{n}$ \\
        \bigbreak
        $\Vert x^{(k)} - x \Vert_2 = \Vert x^{(k)} \Vert_2 = (\sum_{i=1}^{n} (\frac{i}{k})^2)^\frac{1}{2} = \frac{1}{k} (\sum_{i=1}^{n} i^2)^\frac{1}{2}$ \\
        $\underset{k \to \infty}{\lim} \Vert x^{(k)} - x \Vert_2 = \underset{k \to \infty}{\lim} \frac{1}{k} (\sum_{i=1}^{n} i^2)^\frac{1}{2} = 0 $ \\
        $x^{(k)} \to x$
    \end{center}
Unfortunately, the limit of a sequence may not always be in the same vector space as the original sequence. If this happen, we call this the normed vector space incomplete. Otherwise, it is a complete vector space also known as the Banach space.

\bigbreak

Example of Banach space:
\begin{enumerate}
    \item $\mathbb{R}^{n}$ with any norm
    \item  $\mathbb{R}^{m \times n}$ with any norm
    \item Tensor space $\mathbb{R} ^ {m \times n \times l}$ with any norm
    \item $C[a,b]$ with $\Vert \cdot \Vert_\infty$
    \item $L_p$ with p-norm, for $p \geq 1$ and $p = \infty$.
\end{enumerate}

\bigbreak

\textbf{Cauchy Sequence} \\
\textbf{Definition:} $\{x^{(k)}\}$ is a Cauchy sequence, if for any $\epsilon >  0$, there exists K such that for any $k,l >  K$, $\Vert x^{(k)} - x^{(l)} \Vert <  \epsilon$.  \\
    Facts: 
    \begin{enumerate}
        \item If $x^{(k)} \to x$ in $(\mathbb{V},\Vert \cdot \Vert)$, then $\{x^{(k)}\}$ must also be a Cauchy sequence. \\
        \textbf{Proof.} \\
        $x^{(k)} \to x$ implies that $\forall \epsilon >  0$, $\exists k$, s.t. $k >  K \ \Vert x^{(k)} - x \Vert \leq \frac{\epsilon}{2}$. Therefore, $\Vert x^{(k)} - x^{(l)} \Vert \leq \Vert x^{(k)} - x \Vert + \Vert x^{(l)} - x \Vert \leq \epsilon$, $\forall k, l >  K$
        \pagebreak
        \item The reverse is \textbf{NOT} necessarily true. 
    \end{enumerate}
    
\textbf{Definition:} A vector space $(\mathbb{V},\Vert \cdot \Vert)$ is complete if the limit of all Cauchy sequences in $\mathbb{V}$ is in $\mathbb{V}$. \\


\bigbreak

\textbf{Remarks:} We can always complete an incomplete normed vector space by including all limits of its Cauchy sequence.

\bigbreak 

\textbf{Finite Dimensional Vector Space} \\
In most cases, we are dealing with finite dimensional vector space such as $\mathbb{R}^{n}$, $\mathbb{R}^{m \times n}$ and $\mathbb{R}^{m \times n \times l}$. \\
\bigbreak
Properties related to Finite Dimensional Vector Space:\\
    \begin{itemize}
        \item Any finite dimensional vector space with any norm is complete. That is, any finite dimensional vector space is Banach space.
        \item For a finite dimensional vector space $\mathbb{V}$, all norms are equivalent. 
        \textbf{Theorem:} For any norms $\Vert \cdot \Vert_A$ and $\Vert \cdot \Vert_B$, $\exists c_1, c_2 >  0$ s.t. $c_1 \Vert a \Vert_A \leq \Vert a \Vert_B \leq c_2\Vert a \Vert_A$, $\forall a \in \mathbb{V}$ (finite dimensional) \\  
    \end{itemize}
    
\textbf{Example:} Prove that $x^{(k)} \to x$ in $\Vert \cdot \Vert_A \iff x^{(k)} \to x$ in $\Vert \cdot \Vert_B$. \\
    Since $x^{(k)} \to x$ in $\Vert \cdot \Vert_A$, \\
    \begin{center}
            $\underset{k \to \infty}{\lim} \Vert x ^ {(k)} - x \Vert_A = 0 $ \\
            \raggedright Because of equivalence, \\
            \begin{center}
                $c_1 \Vert x^{(k)} - x \Vert_A \leq \Vert x^{(k)} - x \Vert_B \leq c_2 \Vert x^{(k)} - x \Vert_A $ \bigbreak
                $0 \leq \underset{k \to \infty}{\lim}\Vert x^{(k)} - x \Vert_B \leq c_2 \underset{k \to \infty}{\lim}\Vert  x^{(k)} - x \Vert_A = 0$ \bigbreak
                $\underset{k \to \infty}{\lim} \Vert x^{(k)} - x \Vert_B = 0$ (by squeeze theorem) \bigbreak
                $x^{(k)} \to x$ under $\Vert \cdot \Vert_B$
            \end{center}
    Similarly for the $\leftarrow$ direction.
    \end{center}

\textbf{Example:} Consider $\mathbb{R}^{n}$ and $\Vert \cdot \Vert_1$, $\Vert \cdot \Vert_2$ and $\Vert \cdot \Vert_\infty$.  \\
\begin{itemize}
    \item $\Vert \cdot \Vert_1$ and $\Vert \cdot \Vert_2$ are equivalent. \\
        \begin{center}
            $\Vert a \Vert_2 \leq \Vert a \Vert_1 \leq \sqrt{n}\Vert a \Vert_2$, $\forall a \in \mathbb{R}^{n}$
        \end{center}
    \item $\Vert \cdot \Vert_2$ and $\Vert \cdot \Vert_\infty$ are equivalent.
        \begin{center}
            $\Vert a \Vert_\infty \leq \Vert a \Vert_2 \leq \sqrt{n}\Vert a \Vert_\infty$, $\forall a \in \mathbb{R}^{n}$
        \end{center}
    \item $\Vert \cdot \Vert_1$ and $\Vert \cdot \Vert_\infty$ are equivalent.
        \begin{center}
            $\Vert a \Vert_\infty \leq \Vert a \Vert_1 \leq n \Vert a \Vert_\infty$, $\forall a \in \mathbb{R}^{n}$
        \end{center}
\end{itemize}
\textbf{Remarks:} Though they are equivalent, the speed at which they converge are different. In other words, the convergence speed depends on norms. 

\pagebreak
\subsection*{Case Study: Clustering, k-means, k-medians}
\textbf{Clustering} \\
Suppose we are given $N$ vectors $x_1, x_2, \cdots, x_N \in \mathbb{R}^{n}$, the goal of clustering is to group or partition the vectors into $k$ groups or clusters, with the vectors in each group close to each other.

\begin{center}
        \fbox{\includegraphics[width=6cm, height=4cm]{14.jpeg}}
\end{center}

We use $\mathbb{R}^{n}$ because it is simple, yet able to model a variety of data sets (e.g., signals, images, videos, attributes of things). Actually, the methods can be extended to any normed vector spaces (Banach space).
Applications:
\begin{itemize}
    \item Recommendation system
    \item Image clustering
    \item Text data clustering
    \item Many other applications.
\end{itemize}



\textbf{Mathematical formulation:}
\begin{itemize}

\item \textbf{Representation:} \\
Let $c_i \in \{1,2,\cdots,k\}$ be the group that $x_i$ belongs to. $i = 1,2, \cdots, N$ \\
Then, group $G_j$ denoted by $G_j$, is $G_j = \{i|c_i = j\}$. $j = 1,2, \cdots,k$. \\
We assign each group a representative vector, denoted by $z_1, z_2, \cdots, z_k$. The representative vectors are not necessarily one of the given vectors.

\item \textbf{Evaluation:} \\
First of all, within one specific group $G_j$, all vectors should be close to the representative vector $z_j$. More precisely, let 
\begin{center}
    $J_j = \sum_{i \in G_j} \Vert x_i - z_j \Vert_{2}^{2}$
\end{center}
Then, $J_j$ should be small. \\
Secondly, consider all groups, since each $J_j$ is small,
\begin{center}
    $J = J_1 + J_2 + \cdots + J_k$
\end{center}
should be small. \\
\pagebreak
Altogether, we solve the following
\begin{center}
    $\underset{\underset{z_1, \cdots, z_k}{G_1, \cdots, G_k}}{\min} J \iff 
    \underset{\underset{z_1, \cdots, z_k}{G_1, \cdots, G_k}}{\min} \sum_{j=1}^{k} $J_j$ \iff \underset{\underset{z_1, \cdots, z_k}{G_1, \cdots, G_k}}{\min} \sum_{j=1}^{k} (\sum_{i \in g_j} \Vert x_i - z_j \Vert_{2}^{2})$ 
\end{center}
\item \textbf{Optimization:}  \\
We may use an alternating minimization to solve this minimization problem. 
\bigbreak
Step 1: Fix the representative $z_1, \cdots, z_k$, find the best partitions $G_1, ..., G_k$, i.e., solve 
\begin{center}
    $\underset{G_1,\cdots,G_k}{\min} \sum_{j=1}^{k} (\sum_{i \in G_j} \Vert x_i - z_j \Vert_{2}^{2})$ \textcircled{1}
\end{center}
Step 2: Fix the groups $G_1, \cdots, G_K$, find the best representatives $z_1, \cdots, z_k$, i.e., solve
\begin{center}
    $\underset{z_1, \cdots, z_k}{\min} \sum_{j=1}^{k} (\sum_{i \in G_j} \Vert x_i - z_j \Vert_{2}^{2})$ \textcircled{2}
\end{center}
The two steps are repeated until convergence.

\end{itemize}
Let's find the solutions of the sub-problems \textcircled{1} and \textcircled{2} respectively. 

\bigbreak

For \textcircled{1}: \\
Finding the partition $G_1, \cdots, G_k$ is equivalent to finding $c_1, \cdots, c_N$. So \textcircled{1} becomes
\begin{center}
    $\underset{c_1, \cdots, c_N}{\min}(\Vert x_1 - z_{c_1} \Vert_{2}^{2} + \cdots + \Vert x_N - z_{c_N} \Vert_{2}^{2})$ \\
    $\iff \underset{c_i \in \{1,2,\cdots,k\}}{\min} \Vert x_i - z_{c_i} \Vert_{2}^{2}$ \\
    $\iff c_i = \underset{j \in \{1, 2,\cdots, k\}}{\text{argmin}} \Vert x_i - z_j \Vert_{2}^{2}$
    
\end{center}

In other words, $x_i$ is assigned to the group whose representative vector is the closest to $x_i$.

\bigbreak

For \textcircled{2}: \\
It is rewritten as 
\begin{center}
     $\underset{z_1, \cdots, z_k}{\min}(\sum_{i \in G_i}\Vert x_i - z_1 \Vert_{2}^{2} + \cdots + \sum_{i \in G_k}\Vert x_i - z_k\Vert_{2}^{2})$ \\
\end{center}
Obviously, it is equivalent to minimize each term independently, i.e., solve $k$ independent problems.
\begin{center}
    $\underset{z_j}{\min}(\sum_{i \in G_j} \Vert x_i - z_j \Vert_{2}^{2})$, $j = 1,2, \cdots, k$ \\
    $\iff z_j = \frac{1}{\mid{G_j}\mid} \sum_{i \in G_j} x_i$, $j = 1,2, \cdots, k$ \\
    where $\mid{G_j}\mid$ is the number of elements in $G_j$. 
\end{center}
In other words, $z_j$ is the mean of all vector in $G_j$.
Note that in the above derivation, when we consider n = 1,
\begin{center}
    $\underset{z_j \in \mathbb{R}}{\min} \sum_{i \in G_j} (x_i - z_j)^2$ \\
    $\iff \sum_{i \in G_j}2(x_i - z_j) = 0$ \\
    $\iff \sum_{i \in G_j} z_j = \sum_{i \in G_j} x_i$ \\
    $\iff z_j = \frac{1}{\mid{G_j}\mid} \sum_{i \in G_j} x_i$ 
    
    
\end{center}
Altogether, we get the following clustering algorithm. \\
\bigbreak
\textbf{k-means Clustering} \\
Initialization: Initialize $z_1, z_2, \cdots, z_k$. \\
Step 1: Given $z_1,z_2, \cdots, z_k$, compute
\begin{center}
    $c_i = \underset{j \in \{1,2,\cdots,k\}}{\text{argmin}}\Vert x_i - z_j \Vert_{2}^{2}$, $i = 1,2, \cdots, N$ 
\end{center}
and define 
\begin{center}
    $G_j = \{i|c_i = j\}$, $j = 1,2, \cdots, k$
\end{center}
Step 2: Given $G_1,G_2,\cdots,G_k$, compute
\begin{center}
    $z_j = \frac{1}{\mid{G_j}\mid} (\sum_{i \in G_j} x_i)$
\end{center}
Go back to step 1. 
\bigbreak

In k-means, the Euclidean norm is used. We can replace it by 1-norm. We solve 
\begin{center}
    $\underset{\underset{z_1, \cdots, z_k}{G_1, \cdots, G_k}}{\min} \sum_{j=1}^{k} \Vert x_i - z_j \Vert_1$
\end{center}
\textbf{k-medians Clustering} \\
Initialization: Initialize $z_1, z_2, \cdots, z_k$. \\
Step 1: Given $z_1,z_2, \cdots, z_k$, compute
\begin{center}
    $c_i = \underset{j \in \{1,2,\cdots,k\}}{\text{argmin}}\Vert x_i - z_j \Vert_{1}$, $i = 1,2, \cdots, N$ 
\end{center}
and define 
\begin{center}
    $G_j = \{i|c_i = j\}$, $j = 1,2, \cdots, k$
\end{center}
Step 2: Given $G_1,G_2,\cdots,G_k$, compute
\begin{center}
    $z_j = median\{x_i | i \in G_j\}$
\end{center}
Go back to step 1.


\pagebreak



\subsection*{2.3 Inner Product and Hilbert Space}
\textbf{Question:} How do we describe the correlation/centerment between two vectors? Norms are not able to describe it as they are 'scaling sensitive'. \\

\bigbreak

A good answer would be to use angle. A good candidate would be to use inner product since it is 'scaling insensitive'. 

\bigbreak

\textbf{Inner Product} \\
\textbf{Definition:} A function $\langle \cdot,\cdot\rangle : (\mathbb{V},\mathbb{V}) \to \mathbb{R}$ on a vector space $\mathbb{V}$ is called an inner product over $\mathbb{R}$, if: 
    \begin{enumerate}
        \item $\forall x \in \mathbb{V}, \langle x,x\rangle  \ \geq 0$ and  $\langle x,x\rangle  \ = \ 0 \iff x = 0$
        \item $\langle \alpha x_1 + \beta x_2, y\rangle  = \alpha \langle x_1,y\rangle  + \beta \langle x_2,y\rangle,  \  \forall \alpha, \beta \in \mathbb{R}, x_1, x_2, y \in \mathbb{V}$
        \item $\langle x,y\rangle  = \langle y,x\rangle, \ \forall x, y \in \mathbb{V}$
    \end{enumerate}

\textbf{Remarks: } \begin{enumerate}
    \item By $2$ and $3$, $\langle x,\alpha y_1 + \beta y_2\rangle  = \alpha\langle x,y_1\rangle  + \beta\langle x,y_2\rangle $, $\forall \alpha \beta \in \mathbb{R}$, $x_1,y_1,y_2 \in \mathbb{V}$. Therefore, $\langle \cdot,\cdot\rangle $ is a bi-linear function, i.e., it is linear with respect to one of the variable with the other fixed. 
      \item For inner product of vector spaces on $\mathbb{C}$, we only need to change $3$ to $\langle x,y\rangle  = \overline{\langle y,x \rangle }$, where $\overline{\langle \cdot, \cdot \rangle }$ stands for complex conjugate.
\end{enumerate}

    

\textbf{Example:} $\mathbb{R}^{n}$ is a vector space. We can define an inner product as 
    \begin{center}
        $\langle x,y\rangle  = \sum_{i=1}^{n} x_i y_i = x^Ty$, $\forall x,y \in \mathbb{R}^{n}$.
      \end{center}

\bigbreak

\textbf{Example:} Another inner product in $\mathbb{R}^{n}$ is as follows. We can define a "weighted" inner product as $\langle x,y\rangle _A = x^TAy$, where $A \in \mathbb{R}^{n \times n}$ is a symmetric positive definite matrix. 

\textbf{Remarks:} A is SPD $\iff A = A^T$ and $x^TAx >  0$ $\forall x \in \mathbb{R}^{n}$ and $x \neq 0$.

\bigbreak

\textbf{Example:} $\mathbb{R}^{m \times n}$ is a vector space. We can define an inner product as 
    \begin{center}
        $\langle A,B\rangle  = \sum_{i=1}^{m} \sum_{j=1}^{n} a_{ij}b_{ij}$, $\forall A,B \in \mathbb{R}^{m \times n}$ \\
    \end{center}
    \bigbreak
        Similarly, these are equal to $trace(A^TB),\ trace(B^TA),\ trace(AB^T) $ and $trace(BA^T)$, where $trace(A)$ is defined as the sum of the diagonal of matrix A. 

\bigbreak
\textbf{Example:} In $L_2$, we can define an inner product as \\
    \begin{center}
        $\langle a,b\rangle  = \sum_{i=1}^{\infty} a_i b_i$, $\forall a,b \in L_2$
    \end{center}

\bigbreak
    
\textbf{Example:} In $C[a,b]$, we can define an inner product as
    \begin{center}
        $\langle f,g\rangle  = \int_{a}^{b} f(t)g(t) dt$, $\forall f,g \in C[a,b]$
    \end{center}
    
\pagebreak

\textbf{Cauchy-Schwartz Inequality} \\
If $\langle \cdot,\cdot\rangle $ is an inner product on $\mathbb{V}$, then, for any $x,y \in \mathbb{V}$, \begin{center} $\mid{\langle x,y\rangle }\mid^2 \ \leq \ \langle x,x\rangle \langle y,y\rangle $ 
\end{center} \\The equality holds true if and only if $x = \alpha y$ or $y = \alpha x$ for some $\alpha \in \mathbb{R}$ \\
\bigbreak
\textbf{Proof.} \\
Let $\lambda \in \mathbb{R}$ be an arbitrary number, \\
\begin{center}
$0  \ \leq \ \langle x + \lambda y, x + \lambda y\rangle  \bigbreak  = $\langle x,x\rangle $ + \lambda \langle y,x\rangle  + \lambda \langle x,y\rangle  + \lambda^2 \langle y,y\rangle  \bigbreak = $\langle x,x\rangle $ + 2 \lambda \langle x,y\rangle  + \langle x,x\rangle $ \bigbreak Thus, $\lambda^2 \langle y,y\rangle  + 2 \lambda \langle x,y\rangle  + \langle x,x\rangle  \ \geq \ 0$, $\forall \lambda \in \mathbb{R}$
\end{center}

The left is a quadratic function of $\lambda$ and is always non-negative. There is at most one root of the quadratic function, hence, the determinant $b^2 - 4ac \leq 0$.

\begin{center}
    So, $(2\langle x,y\rangle )^2 - 4\langle x,x\rangle \langle y,y\rangle  \leq 0$ 
    \bigbreak
    $\implies \langle x,y\rangle ^2 \ \leq \ \langle x,x\rangle  \langle y,y\rangle $  
\end{center} 

Finally, when $\langle x,y\rangle ^2 = \langle x,x\rangle \langle y,y\rangle $, there is a root, i.e., $\exists$ a unique $\lambda \in \mathbb{R}$, $\lambda^2 \langle y,y\rangle  + 2\lambda\langle x,y\rangle  + \langle x,x\rangle  = 0$.
\begin{center} 
    \iff 
    \bigbreak
    $\exists$ a unique $\lambda \in \mathbb{R}$, $\langle x + \lambda y,x+ \lambda y\rangle  = 0$.
    \bigbreak
    \iff 
    \bigbreak
    $\exist$ a unique $\lambda \in \mathbb{R}$, $x + \lambda y = 0$.
    \bigbreak
    \iff
    \bigbreak
    $\exists$ a unique $\lambda \in \mathbb{R}$, $x = -\lambda y$.
\end{center}
\pagebreak

With the Cauchy-Schwartz inequality, we can show that 
\begin{center}
    $\Vert x \Vert = (\langle x,x\rangle )^\frac{1}{2}$ defines a norm.
\end{center}
This is also called "norm induced by the inner product". This one above is for $\mathbb{R}^{n}$.\\
\bigbreak
\textbf{Proof.} \\
\begin{center}
    \item  $\Vert x \Vert = (\langle x,x\rangle )^\frac{1}{2} \geq 0$ and
    $\Vert x \Vert = (\langle x,x\rangle )^\frac{1}{2} = 0 \iff x = 0$ 
    \bigbreak
    \item $\Vert \alpha x \Vert = (\langle \alpha x, \alpha x\rangle )^\frac{1}{2} = (\alpha^2 \langle x,x\rangle )^\frac{1}{2} = \mid{\alpha}\mid \Vert x \Vert$ 
    \bigbreak
    \item $\Vert x + y \Vert^2 = \langle x + y, x + y\rangle  \bigbreak
    = $\langle x,x\rangle $ + $\langle x,y\rangle $+ $\langle y,x\rangle $ + $\langle y,y\rangle $ 
    \bigbreak
    = \Vert x \Vert^2 + \Vert y \Vert^2 + 2\langle x,y\rangle  
    \bigbreak 
    \leq \Vert x \Vert^2 + \Vert y \Vert^2 + 2 \Vert x \Vert \Vert y \Vert 
    \bigbreak 
    =(\Vert x \Vert + \Vert y \Vert)^2 
    \bigbreak
    \Vert x + y \Vert \leq \Vert x \Vert + \Vert y \Vert$ 
    
\end{center}
\textbf{Remarks:} In the proof above, we have used an alternative version of the Cauchy-Schwartz inequality. 
    \begin{center}
        $\mid{\langle x,y\rangle }\mid \ \leq \ \Vert x \Vert \Vert y \Vert$
    \end{center}
    
    
\textbf{All kinds of induced norm} \\
\begin{enumerate}
    \item $\mathbb{R}^{n}$ with inner product $\langle \cdot,\cdot\rangle : \ \langle x,y\rangle  = x^Ty$ \\
    The induced norm is \\
    $\Vert x \Vert = (\langle x,x\rangle )^\frac{1}{2} = (x^Tx)^\frac{1}{2} = (\sum_{i=1}^{n} x_i^2)^\frac{1}{2} = \Vert x \Vert_2$
    \item $\mathbb{R}^{n}$ with weighted inner product $\langle \cdot,\cdot\rangle _A: \ \langle x,y\rangle _A = x^TAy$  \\
    The induced norm is \\
    $\Vert x \Vert_A = (x^TAx)^\frac{1}{2} = (\sum_{i=1}^{n} \sum_{j=1}^{n} a_{ij}x_ix{j})$
    \item The p-norm of $\mathbb{R}^{n}$ \\
    $\Vert x \Vert_p = (\sum_{i=1}^{n} \mid{x_i} \mid^p)^\frac{1}{p} $ \\
    When $p$ = 2, $\Vert \cdot \Vert_2$ is induced by $\langle \cdot,\cdot\rangle $. It is not induced by inner product for all $p$ except for 2.
    \item $\mathbb{R}^{m \times n}$ with inner product $\langle \cdot,\cdot\rangle :$ \ $\langle A,B\rangle  = \sum_{ij} a_{ij} b{ij}}$ \\
    The induced norm is \\
    $\Vert A \Vert = (\langle A,A\rangle )^\frac{1}{2} = (\sum_{ij} a_{ij}^2)^\frac{1}{2} = \Vert A \Vert_F = \Vert A \Vert_{vec, 2}$
    \item Infinite sequence with inner product $\langle \cdot,\cdot\rangle : \ \langle a,b\rangle  = \sum_{i=1}^{\infty} a_i b_i$ \\
    $\Vert a \Vert = (\sum_{i=1}^{\infty} a_i^2)^\frac{1}{2} = \Vert a \Vert_2$
    \item $C[a,b]$ with inner product $\langle \cdot,\cdot\rangle : \ \langle f,g\rangle  = \int_{a}^{b} f(t) g(t) dt$ \\
    $\Vert f \Vert = (\int_{a}^{b} (f(t))^2 dt)^\frac{1}{2} = \Vert f \Vert_2$
\end{enumerate}

\pagebreak

\textbf{Angle in inner product spaces} \\
By Cauchy-Schwartz inequality, 
\begin{center}
    $\mid{\langle x,y\rangle }\mid \leq \Vert x \Vert \Vert y \Vert \ \forall x,y \in \mathbb{V}$ \\
\end{center}
    Then, \\
    \begin{center}
        $-\Vert x \Vert \Vert y \Vert \ \leq \ \langle x,y\rangle  \ \leq \ \Vert x \Vert \Vert y \Vert$ \bigbreak
        $-1 \ \leq \ \frac{\langle x,y\rangle }{\Vert x \Vert \Vert y \Vert}$ \ \leq \ 1$ if $x,y \neq 0$ \\
        
    \end{center}
If $\frac{\langle x,y\rangle }{\Vert x \Vert \Vert y \Vert} = 1$, then $x = \alpha y$ with $\alpha >  0$. Otherwise, if $\alpha \leq 0$, then $\langle x,y\rangle  = \alpha\langle y,y\rangle  = \alpha \Vert y \Vert^2 \leq 0. \ (Contradiction)$. 
\begin{center}
    \fbox{\includegraphics[width=5cm, height=2cm]{6.jpeg}}
\end{center}
If $\frac{\langle x,y\rangle }{\Vert x \Vert \Vert y \Vert} = -1$, then $x = \alpha y$ with $\alpha <  0$.
\begin{center}
    \fbox{\includegraphics[width=5cm, height=2cm]{7.jpeg}}
\end{center}
If $ -1 \ <  \ \frac{\langle x,y\rangle }{\Vert x \Vert \Vert y \Vert} \ <  \ 1$, then
\begin{center}
    \fbox{\includegraphics[width=6cm, height=2cm]{8.jpeg}}
\end{center}
Then we define
    \begin{center}
        \fbox{$\textit{L}(x,y) = \arccos{\frac{\langle x,y\rangle }{\Vert x \Vert \Vert y \Vert}}$}
    \end{center}
This definition is consistent with the observation above and the angles of vectors in $\mathbb{R}^{2}$ and $\mathbb{R}^{3}$.
\pagebreak

\textbf{Orthogonality} \\
Let $\mathbb{V}$ be a vector space and $\langle \cdot,\cdot\rangle $ be the inner product.
\begin{itemize}
    \item If $\frac{\langle x,y\rangle }{\Vert x \Vert \Vert y \Vert} = 1$ or -1, then x and y are the most correlated.
    \item If $\frac{\langle x,y\rangle }{\Vert x \Vert \Vert y \Vert} = 0$, then x and y are the least correlated.
\end{itemize}
If $\langle x,y\rangle  = 0$, then we say x and y are orthogonal. 
\begin{center}
    \fbox{\includegraphics[width=10cm, height=4cm]{9.jpeg}}
\end{center}

\textbf{Pythagorean theorem} \\ 
\textbf{Definition:} Let $x$, $y$ be two vectors in an inner product space $\mathbb{V}$. \\ Then $x \perp y \iff \Vert x + y \Vert^2 = \Vert x \Vert^2 + \Vert y \Vert^2$. \\
\textbf{Proof.} \\
\begin{center}
    $\Vert x + y \Vert_2 = \langle x + y, x + y\rangle  \bigbreak = \Vert x \Vert^2 + \Vert y \Vert^2 + 2\langle x,y\rangle  \textbf{(1)}$ 
\end{center}
If $x \perp y$, then $\langle x,y\rangle  = 0$.
\begin{center}
    $\implies \Vert x + y \Vert^2 = \Vert x \Vert^2 + \Vert y \Vert^2$ \\
\end{center}
If $\Vert x + y \Vert^2 = \Vert x \Vert^2 + \Vert y \Vert^2$, together with \textbf{(1)}, we have $\langle x,y\rangle  = 0$. 

\pagebreak
\textbf{Hilbert Space} \\
\textbf{Definition:} A Hilbert space is a Banach space in which the norm is induced by an inner product. 
\begin{center}
    \fbox{\includegraphics[width=10cm, height=4cm]{10.jpeg}}
\end{center}
\textbf{Examples of Hilbert Space}
\begin{enumerate}
    \item $\mathbb{R}^{n}$ with $\langle \cdot,\cdot\rangle $ is a Hilbert space.
    \item $\mathbb{R}^{n}$ with $\langle \cdot,\cdot\rangle _A$ is a Hilbert space.
    \item $\mathbb{R}^{m \times n}$ with $\langle \cdot,\cdot\rangle $ is a Hilbert space.
    \item $L_2 = \{a \ | \ \Vert a \Vert_2 < \infty \text{ and a is a infinite sequence}\}$ with $\langle \cdot,\cdot\rangle $ is a Hilbert space.
    \item $C[a,b]$ with $\langle \cdot,\cdot\rangle $ is \textbf{NOT} a Hilbert space, because it is not a Banach space. In other words, the limit of a convergent sequence in $C[a,b]$ may not be in $C[a,b]$. To complete $C[a,b]$ under the norm $\Vert \cdot \Vert = (\langle \cdot,\cdot\rangle )^\frac{1}{2}$, we need to extend the Riemann integral to the so-called Lebesgue integral,  and the resulting Hilbert space is $L^2(a,b)$. 

\end{enumerate}

In the following chapters, we will consider calculus on Hilbert/Banach spaces.

\pagebreak
\subsection*{Case Study: Kernel trick, Kernel k-means} \\
Recall that in k-means, we want to group $x_1, x_2, \cdots, x_N \in \mathbb{R}^{n}$ into $k$ groups. k-means work well only if the data are linearly separable. It will fail on "curved" data sets in $\mathbb{R}^{n}$.

\bigbreak

k-means will fail for the following examples.  
\begin{center}
    \fbox{\includegraphics[width=10cm, height=3cm]{15.jpeg}}
\end{center}

To modify k-means to these "curved" data sets in $\mathbb{R}^{n}$, we transform the "curved" data sets to "uncurved" data set in a Hilbert space $\mathbb{H}$. 

Let $\phi: \mathbb{R}^{n} \to \mathbb{H}$,
\begin{center}
    \fbox{\includegraphics[width=10cm, height=3cm]{16.jpeg}}
\end{center}

Then we apply k-means to $\phi(x_1), \phi(x_2), \cdots, \phi(x_N)$ in $\mathbb{H}$. Let $z_1,z_2,\cdots,z_k$ be the representative vectors in $\mathbb{H}$. 

\bigbreak

\textbf{k-means Clustering} \\
Step 0: Initialize $z_1,z_2,\cdots,z_k$ \\
Step 1: Given $z_1,z_2, \cdots, z_k$, compute
\begin{center}
    $c_i = \underset{j \in \{1,2,\cdots,k\}}{\text{argmin}}\Vert \phi(x_i) - z_j \Vert^{2}$, $i = 1,2, \cdots, N$ 
\end{center}
and define 
\begin{center}
    $G_j = \{i|c_i = j\}$, $j = 1,2, \cdots, k$
\end{center}
Step 2: Given $G_1,G_2,\cdots,G_k$, compute
\begin{center}
    $z_j = \frac{1}{\mid{G_j}\mid} (\sum_{i \in G_j} \phi(x_i))$
\end{center}
Go back to step 1. 

\pagebreak
However, finding the feature map $\phi$ is not easy, because $\phi$ depends on the shape of $x_1,x_2,\cdots,x_N$, which generally is very complicated. \\
The good news is that:
\begin{center}
    \fbox{There is no need to know $\phi$ explicitly in k-means algorithm.}
\end{center}
Why?
\begin{itemize}
    \item First of all, since we care only about the groups of $x_1, \cdots, x_N$, we only need to know $G_1, \cdots, G_k$. The representatives $z_1, \cdots, z_k$ are only intermediate. Therefore, we can eliminate $z_1, \cdots, z_k$ in the k-means algorithm. 
    \bigbreak
    \textbf{Modified k-means Clustering} \\
    Step 0: Initialize $G_1, \cdots, G_k$ \\
    Step 1: Given $G_1, \cdots, G_k$, compute
\begin{center}
    $c_i = \underset{j \in \{1,2,\cdots,k\}}{\text{argmin}}\Vert \phi(x_i) - \frac{1}{\mid{G_j}\mid} \sum_{l \in G_j} \phi(x_l) \Vert^{2}$, $i = 1,2, \cdots, N$ 
\end{center}
and define 
\begin{center}
    $G_j = \{i|c_i = j\}$, $j = 1,2, \cdots, k$
\end{center}
Go back to step 1.
\item Now, since we are talking about Hilbert space $\mathbb{H}$, we can expand the norm by 
\begin{center}
    $\Vert \phi(x_i) - \frac{1}{\mid{G_j}\mid} \sum_{l \in G_j} \phi(x_l) \Vert^{2}$ \\
    $= \langle \phi(x_i) - \frac{1}{\mid{G_j}\mid} \sum_{l \in G_j} \phi(x_l), \phi(x_i) - \frac{1}{\mid{G_j}\mid} \sum_{l \in G_j} \phi(x_l) \rangle$ \\
    $= \langle \phi(x_i), \phi(x_i) \rangle - \frac{2}{\mid G_j \mid} \sum_{l \in G_j} \langle \phi(x_i), \phi(x_l) \rangle + \frac{1}{\mid G_j \mid^2} \sum_{l_1 \in G_j} \sum_{l_2 \in G_j} \langle \phi(x_{l_1}), \phi(x_{l_2}) \rangle$  
    
\end{center}
All terms involved are in the form of 
\begin{center}
    $\langle \phi(x), \phi(y) \rangle : (\mathbb{R}^{n}, \mathbb{R}^{n}) \to \mathbb{R}$
\end{center}
 
Instead of defining $\phi$ explicitly, we define a function 
\begin{center}
     $\langle \phi(x), \phi(y) \rangle : (\mathbb{R}^{n}, \mathbb{R}^{n}) \to \mathbb{R}$ 
\end{center}
s.t. $k(x,y) = \langle \phi(x), \phi(y) \rangle$. 
\bigbreak
Therefore, an explicit expression of $\phi$ is \textbf{NOT} necessary. This process is also known as kernel trick. 
\end{itemize}
\pagebreak
\textbf{Kernel function} \\
$k(x,y)$ is called a kernel function. Which kernel function? \\
Necessary conditions:
\begin{enumerate}
    \item $k(x,y) = \langle \phi(x), \phi(y) \rangle = \langle \phi(y), \phi(x) \rangle = k(y,x)$ \\ We say $k$ is a symmetric kernel is $k(x,y) = k(y,x)$, $\forall x, y \in \mathbb{R}^{n}$.
    \item Let $y_1, \cdots, y_m \in \mathbb{R}^{n}$. Then for any $C = \begin{bmatrix} c_1 \\ c_2 \\ \vdots \\ c_m \end{bmatrix} \in \mathbb{R}^{n}$, 
    \begin{center}
        $0 \leq \langle \sum_{i=1}^{m} c_i \phi(y_i), \sum_{j=1}^{m} c_j \phi(y_j) \rangle = \sum_{i=1}^{m} \sum_{j=1}^{m} c_i c_j \langle \phi(y_i), \phi(y_j) \rangle = $\\ 
        $\sum_{i=1}^{m} \sum_{j=1}^{m} c_i c_j k(y_i, y_j) = C^TkC$ where $k = [k(y_i,y_j)]_{i,j}$  
    \end{center}
\end{enumerate}
That is, $\forall C \in \mathbb{R}^{m}$, $C^TkC \geq 0$ and $k^Tk$.
\bigbreak

\textbf{Definition:} We say a kernel function $K: (\mathbb{R}^n, \mathbb{R}^{n}) \to \mathbb{R}$ is symmetric positive semi-definite if:
\begin{enumerate}
    \item $k(x,y) = k(y,x)$, $\forall x,y \in \mathbb{R}^{n}$
    \item For any m and $y_1, y_2, \cdots, y_m \in \mathbb{R}^{n}$, the matrix 
    \begin{center}
        $k = [k(y_i, y_j)]_{i,j}$ 
    \end{center}
    is symmetric positive semi-definite.
    
\end{enumerate}

\textbf{Mercer's Theorem:} If $K: (\mathbb{R}^n, \mathbb{R}^{n}) \to \mathbb{R}$ is continuous and symmetric positive semi-definite, then there exists a Hilbert space $\mathbb{H}$ and a mapping such that $k(x,y) = \langle \phi(x), \phi(y) \rangle$.

\bigbreak

Some popular kernels:
\begin{itemize}
    \item $k(x,y) = x^Ty$ ($\phi(x) = x$ (No transform kernel))
    \item $k(x,y) = (x^Ty + 1)^\alpha$ ($\alpha$ is an integer (Polynomial kernel))
    \item $k(x,y) = e^{-\frac{\Vert x - y \Vert_2^{2}}{\sigma^2}}$ ($\sigma > 0$ is a parameter (Gaussian kernel))
\end{itemize} 

\bigbreak

\textbf{Kernel k-means Clustering} \\
 Step 0: Initialize $G_1, \cdots, G_k$ \\
    Step 1: Given $G_1, \cdots, G_k$, compute
\begin{center}
    $c_i = k(x_i,x_i) - \frac{2}{\mid G_j \mid} \sum_{l \in G_j} k(x_i,x_l) + \frac{1}{\mid G_j \mid^2} \sum_{l_1 \in G_j} \sum_{l_2 \in G_j} k(x_{l_1},x_{l_2})$ 
\end{center}
and define 
\begin{center}
    $G_j = \{i|c_i = j\}$, $j = 1,2, \cdots, k$
\end{center}
Go back to step 1.

\pagebreak
\textbf{Why kernel k-means work?} \\

\begin{center}
    \fbox{\includegraphics[width=10cm, height=3cm]{17.jpeg}}
\end{center}

Suppose we use Gaussian kernel,
\begin{itemize}
    \item $k(x_i,x_i) = e^{-\frac{\Vert x_i - x_i \Vert_{2}^{2}}{\sigma^2}} = e^{-0} = 1$, $\forall i$ \\
    so all $\phi(x_1), \cdots, \phi(x_N)$ are on unit sphere in \mathbb{H}.
    \item $k(x_i,x_j) \begin{cases} 
      $\approx 1$ if $x_i \approx x_j$ \\
      $\approx 0$ if $\Vert x_i - x_j \Vert_2$ is large  
   \end{cases}$
   Since $\langle \phi(x_i), \phi(x_j) \rangle = k(x,y)$,
   \begin{itemize}
       \item If $x_i \approx x_j$, then $\Vert \phi(x_i) - \phi(x_j) \Vert^2  = \Vert \phi(x_i) \Vert^2 - 2 \langle \phi(x_i),\phi(x_j) \rangle + \Vert \phi(x_j) \Vert^2 \approx 0$ \\ $\implies \phi(x_i) = \phi(x_j)$.
       \item If $\Vert x_i - x_j \Vert_2$ is large, then $\phi(x_i) \perp \phi(x_j)$.
   \end{itemize}
\begin{center}
    \fbox{\includegraphics[width=10cm, height=3cm]{18.jpeg}}
\end{center}
    
\end{itemize}
Thus, kernel k-means work for "curved" data sets which k-means fail.
\pagebreak
\subsection*{Case Study: Metric Learning}
Given a set of data $x_1,x_2,...,x_n \in \mathbb{R}^{n}$ and
$S: (x_i,x_j) \in S$ if $x_i$ and $x_j$ are similar and $D: (x_i,x_j) \in D$ if $x_i$ and $x_j$ are dissimilar. 
\bigbreak
Our goal is to find a "new" metric such that for similar pair, it is close and for dissimilar pair, it is far away. In other words, in metric learning, given a set of data in two groups, we need to find the metric that would differentiate between the two groups. 

\begin{center}
    \fbox{\includegraphics[width=10cm, height=3cm]{19.jpeg}}
\end{center}

\bigbreak
 
\textbf{Representation:} \\
Norm induced by weighted inner product \\
Given $A \in \mathbb{R}^{n \times n}$ is SPD,
\begin{center}
    $\langle x, y \rangle = x^TAy$ \\
    and $\Vert x \Vert_A = (x^TAx)^\frac{1}{2}$

\end{center}

Then finding a metric is the same as finding an SPD matrix A. 
\bigbreak

\textbf{Remarks:} 
\begin{enumerate}
    \item The set of all SPD is \textbf{NOT} closed. 
    \item The closure of the set of all SPD matrices is the set of all SPSD matrices.
    \item If A is SPSD, then $\Vert x \Vert_A$is not a norm because $\Vert x \Vert_A^2 = 0 \iff x^TAx = 0$ cannot implies $x = 0$. 
    \item $\Vert \cdot \Vert_A$ is still a semi-norm:
    \begin{itemize}
        \item $\Vert x \Vert_A \geq 0$
        \item $\Vert \alpha x \Vert_A = \mid{\alpha}\mid \Vert x \Vert_A$
        \item $\Vert x + y \Vert_A \leq \Vert x \Vert_A + \Vert y \Vert_A$
    \end{itemize}
\end{enumerate}

\pagebreak
\textbf{Evaluation:}\\
Which A is  the best?
\begin{enumerate}
    \item For $(x_i,x_j) \in S$, $dist(x_i,x_j)$ should be small
    \begin{center}
        $\underset{(x_i,x_j) \in S}{\sum} \Vert x_i - x_j \Vert_A^2$
    \end{center}
    \item For $(x_i,x_j) \in D$, $dist(x_i,x_j)$ should not be small \\
    Altogether: \\
    \begin{center}
        $\underset{A \in \mathbb{R}^{n \times n}}{\min} \underset{(x_i,x_j) \in S}{\sum} \Vert x_i - x_j \Vert_A^2$ \\
        s.t. $\underset{(x_i,x_j) \in D}{\sum} \Vert x_i - x_j \Vert_A^2 \geq 1$
    \end{center}
\end{enumerate}

\textbf{Optimization:} \\
It is too complicated.
\bigbreak

\textbf{One popular application: "Supervised" clustering}\\
    \begin{itemize}
        \item   Given a data set with partial clustering information,
        \begin{center}
                \fbox{\includegraphics[width=10cm, height=3cm]{20.jpeg}}
        \end{center}
        \item Apply metric learning (i.e. find a distance)
        \item Cluster the points under the newly learned distance metric.
    \end{itemize}
\pagebreak
\section*{Linear and Differentiable Functions}
\subsection*{3.1 Linear Function}
\textbf{Definition:} Let $\mathbb{V}$ be a vector space and $f: \mathbb{V} \to \mathbb{R}$ be a function. $f$ is a linear function if 
\begin{center}
    $f(\alpha x + \beta y) = \alpha f(x) + \beta f(y)$, $\forall \alpha,\beta \in \mathbb{R}$ and $x,y \in \mathbb{V}$
\end{center}

\textbf{Example:} The mean of a vector in $\mathbb{R}^{n}$. \\
\bigbreak 
$\forall x = \begin{bmatrix} x_1 \\ \vdots \\ x_n \end{bmatrix} \in \mathbb{R}^{n}$, $f(x) = \frac{x_1 + x_2 + \cdots + x_n}{n} = \frac{\sum_{i=1}^{n} x_i}{n}$ is a linear function because 
\begin{center}
    $f(\alpha x + \alpha y) = \frac{\sum_{i=1}^{n} (\alpha x_i + \beta y_i)}{n} = \alpha \frac{\sum_{i=1}^{n} x_i}{n} + \beta \frac{\sum_{i=1}^{n} y_i}{n} = \alpha f(x) + \beta f(y)$
\end{center}

\bigbreak

\textbf{Example:} The maximum entry of a vector in $\mathbb{R}^{n}$.
\bigbreak
$\forall x = \begin{bmatrix} x_1 \\ \vdots \\ x_n \end{bmatrix} \in \mathbb{R}^{n}$, $f(x) = \underset{i=1,\cdots,n}{\max} x_i$ is not a linear function. 
\bigbreak
One counter example: 
\begin{center}
$x = \begin{bmatrix} 1 \\ 0 \\ \vdots \\ 0 \\ 0 \end{bmatrix}$, $y = \begin{bmatrix} 0 \\ 1 \\ \vdots \\ 0 \\ 0 \end{bmatrix}$, $\alpha = 1$, $\beta = 1$ \\

$f(\alpha x + \beta y) = f(\begin{bmatrix} 1 \\ 1 \\ \vdots \\ 0 \\ 0 \end{bmatrix}) = 1$ but $\alpha f(x) = 1$ and $\beta f(y) = 1 $ 
\end{center} 
Hence, $f(\alpha x + \beta y) \neq \alpha f(x) + \beta f(y)$. 
\bigbreak

\textbf{Example:} $f: \mathbb{R}^{n} \to \mathbb{R}$ defined by $f(x) = \langle a,x\rangle $, where $a \in \mathbb{R}^{n}$ is a fixed vector in $\mathbb{R}^{n}$ is linear.

\pagebreak

\textbf{Example:} $F: C[-1,1] \to \mathbb{R}$ defined by $F(f) = f(0)$ is linear because
\bigbreak
\begin{center}
    $F(\alpha f + \beta g) = (\alpha f + \beta g)(0) = \alpha f(0) + \beta g(0) = \alpha F(f) + \beta F(g)$
\end{center}

\bigbreak

\textbf{Example:} $F: C[a,b] \to \mathbb{R}$ defined by $F(f) = \int_{a}^{b} f(t) dt$ is linear because 
\bigbreak
\begin{center}
    $F(\alpha f + \beta g) = \int_{a}^{b} (\alpha f +  \beta g)(t) dt$ \bigbreak
    $ = \int_{a}^{b} (\alpha f(t) + \beta g(t)) dt$ \bigbreak
    $ = \alpha \int_{a}^{b} f(t) dt + \beta \int_{a}^{b} g(t) dt$ \bigbreak
    $ = \alpha F(f) + \beta F(g)$

\end{center}

\textbf{Example:} Let $\mathbb{V}$ be an inner product space with inner product $\langle \cdot,\cdot\rangle $. Let $a \in \mathbb{V}$ and $f: \mathbb{V} \to \mathbb{R}$ defined by $f(x) = \langle a,x\rangle $ is linear. 
\bigbreak

\textbf{Example:} A norm function on $\mathbb{V}$ is \textbf{NOT} linear. 
\bigbreak
\textbf{Proof:} 
Let $\Vert \cdot \Vert: \mathbb{V} \to \mathbb{R}$. Then $\Vert -x \Vert = \Vert x \Vert$ by norm property. \\
If $\Vert \cdot \Vert$ is linear, then \\
\begin{center}
    $\Vert -x \Vert = \Vert -x + 0 \cdot x = -1 \Vert x \Vert + 0 \Vert x \Vert = -\Vert x \Vert$ \textit{(Contradiction)}
\end{center}

\pagebreak


\textbf{Properties of Linear Function:} 
\begin{enumerate}
    \item \textit{Homogeneity}:
    \begin{center}
        $f(\alpha x) = \alpha f(x)$, $\forall \alpha \in \mathbb{R}$, $x \in \mathbb{V}$ \bigbreak
        because $f(\alpha x) = f(\alpha x + 0 \cdot y) = \alpha f(x) + 0 \cdot f(y) = \alpha f(x)$ \bigbreak
        Choosing $\alpha = 0$, then we obtain $f(0) = 0$.
    \end{center}
    \item \textit{Additivity}:
    \begin{center}
        $f(x + y) = f(x) + f(y)$ \bigbreak
        $f(\alpha_1 x_1 + \alpha_2 x_2 + \cdots + \alpha_k x_k)$ \bigbreak
        $= \alpha_1 f(x_1) + f(\alpha_2 x_2 + \cdots + \alpha_k x_k)$ \bigbreak
        $= \alpha_1 f(x_1) + \alpha_2 f(x_2) + f(\alpha_3 x_3 + \cdots + \alpha_k x_k) $ \bigbreak
        $= \cdots$ \bigbreak
        $= \alpha_1 f(x_1) + \alpha_2 f(x_2) + \cdots + \alpha_k f(x_k)$        
        
    \end{center}
\end{enumerate}

\textbf{Linear Function on Hilbert Space} \\
For simplicity, let's consider a linear function on $\mathbb{R}^{n}$ equipped with the standard inner product $\langle x,y \rangle = x^T y $ and the induced norm $\Vert x \Vert_2 = (\langle x,x\rangle )^\frac{1}{2}$. 

\begin{itemize}
    \item From one of the examples above, 
        \begin{center}
            For any give $a \in \mathbb{R}^{n}$, the function $f(x) = \langle a,x\rangle $ is linear.
        \end{center}
    \item The reverse is true, i.e.,
        \begin{center}
            Any linear function $f: \mathbb{R}^{n} \to \mathbb{R}$ must be in the form of $f(x) = \langle a,x\rangle $ for some $a \in \mathbb{R}^{n}$.
        \end{center}
\end{itemize}

\pagebreak
We are assuming that $\mathbb{H} = \mathbb{R}^{n}$ for simplicity, but this theorem actually holds for any forms of Hilbert Space $\mathbb{H}$. 
\bigbreak
\textbf{Theorem:} For any linear function $f: \mathbb{R}^{n} \to \mathbb{R}$, there exists a unique $a \in \mathbb{R}^{n}$ s.t. $f(x) = \langle a,x\rangle $, $\forall x \in \mathbb{R}^{n}$.

\bigbreak

\textbf{Proof:} Let $e_1$, $e_2$, .. $e_n$ be the natural basis of $\mathbb{R}^{n}$ where $e_i$ is a vector where the i-th entry is 1 and 0 elsewhere. \bigbreak

$\forall x = \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix} \in \mathbb{R}^{n}$, $ x = x_1 e_1 + x_2 e_2 + \cdots + x_n e_n$ \bigbreak
So $f(x) = f(x_1 e_1 + x_2 e_2 + \cdots + x_n e_n)$ \bigbreak
$ = x_1 f(e_1) + x_2 f(e_2) + \cdots + x_n f(e_n)$ \bigbreak
 $ = \langle \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix}, \begin{bmatrix} f(e_1) \\ f(e_2) \\ \vdots \\ f(e_n) \end{bmatrix}\rangle $ \bigbreak
 $ = \langle a, x\rangle $ where $a = \begin{bmatrix} f(e_1) \\ f(e_2) \\ \vdots \\ f(e_n) \end{bmatrix}$ 

\bigbreak
Now we prove the uniqueness of this theorem. \\
Suppose $a$ is \textbf{NOT} unique, $\exists a,b \in \mathbb{R}^{n}$ s.t. 
\begin{center}
    $f(x) = \langle a,x\rangle  = \langle b,x\rangle $, $\forall x \in \mathbb{R}^{n}$ 
\end{center}
Then choose $x = e_i$, $i = 1, \cdots, n$
\begin{center}
    $f(e_i) = \langle a,e_i\rangle  = \langle b,e_i\rangle  \implies a_i = b_i$, $i = 1, \cdots, n$ \\
    $$\implies a = b$ \textit{(Contradiction)}
\end{center}
Therefore, $a$ must be unique.

\pagebreak

\textbf{Riesz Representation Theorem} 
\bigbreak
Extending previous theorem to the entirety of Hilbert space $\mathbb{H}$.
\bigbreak
\textbf{Theorem:} \\
Let $\mathbb{H}$ be a Hilbert space. Let $f: \mathbb{H} \to \mathbb{R}$. Then f is linear and bounded if and only if $f(x) = \langle a,x\rangle $ for some unique $a \in \mathbb{H}$.

\bigbreak

\textbf{Example:}  We know that mean of a vector on $\mathbb{R}^{n}$ is linear.
\begin{center}
    $f(x) = \frac{x_1 + x_2 + \cdots + x_n}{n} = \langle \frac{1}{n} \begin{bmatrix} 1 \\ 1 \\ \vdots \\ 1 \end{bmatrix}, x\rangle $ 
\end{center}

\textbf{Example:} Let $\mathbb{H}$ be a Hilbert space and $\Vert \cdot \Vert$ is
\textbf{NOT} linear. So there is no such $a \in \mathbb{H}$ s.t. $\Vert x \Vert = \langle a,x\rangle $, $\forall x \in \mathbb{H}$. 

\bigbreak

\textbf{Example:} $\mathbb{R}^{n \times n}$ with inner product
\begin{center}
    $\langle A,B\rangle  = \sum_{i=1}^{n} \sum_{j=1}^{n} a_{ij} b_{ij}$, $\forall A,B \in \mathbb{R}^{n \times n}$
\end{center}
Define $trace(A) = \sum_{i=1}^{n} a_{ii}, \ \forall A \in \mathbb{R}^{n \times n}$ is linear. We have
\begin{center}
    $trace(A) = \langle A,I\rangle $
\end{center}

\bigbreak

\textbf{Remarks:} 
\begin{enumerate}
    \item In finite dimensional Hilbert space, linear $\iff$ linear and bounded.
    \item In infinite dimensional Hilbert space, there exists linear but unbounded function. 
\end{enumerate}

\pagebreak 


\textbf{Example:} $L^2(-1,1)$ - the completion of $C[-1,1]$ under the inner product $\langle f,g\rangle  = \int_{-1}^{1} f(t)g(t) dt$ and $\Vert f \Vert_2 = (\langle f,f\rangle )^\frac{1}{2} = (\int_{-1}^{1} \mid{f(t)}\mid^2 dt)^\frac{1}{2}$. Consider $F(f) = f(0)$, $\forall f \in L^2(-1,1)$ \\
But $F(f)$ is unbounded since 
\begin{center}
    $\exists f \in L^2(-1,1)$ s.t. F(f) = $\infty$ \\
    \bigbreak
    e.g. $f(t) =  \begin{cases} 
      1 & t\neq 0 \ \text{and} \  t \in (-1,1) \\
      \infty & t = 0 \\
   \end{cases}$ 
\end{center}
There exists no inner product representation for $F(f) = f(0)$.

\bigbreak
\textbf{Example:} $L^2(-1,1)$ \\
Consider $G: L^2(-1,1) \to \mathbb{R}$ \\
\begin{center}
    $G(f) = \int_{-1}^{1} f(t) dt$
\end{center}
G is linear. \\
G is bounded because for any $f \in L^2(-1,1)$, $G(f) = \int_{-1}^{1} f(t) dt = \int_{-1}^{1} f(t) \cdot 1 dt = \langle f, 1\rangle   \ \leq \ \Vert f \Vert_2 (\int_{-1}^{1} 1^2 dt)^\frac{1}{2} \ \leq \ 2\Vert f \Vert_2$  \\

\textbf{Riesz} $\implies$ $\exist g \in L^2(-1,1)$ s.t. $G(f) = \langle f,g\rangle $. Indeed, $g(t) = 1$, $\forall t \in (-1,1)$.


\bigbreak

\textbf{Hyperplane} \\
Let $\mathbb{H}$ be a Hilbert space and $a \in \mathbb{H}$. \\
Consider $S_{a,0} = \{x \in H | \langle a,x\rangle  = 0\} \subset \mathbb{H}$, \\
Then $\forall \alpha, \beta \in \mathbb{R}$ and $\forall x,y \in S_{a,0}$ \\
$\langle a, \alpha x + \beta y\rangle  = \alpha\langle a,x\rangle  + \beta\langle a,y\rangle  = 0$. That is, $\alpha x + \beta y \in S_{a,0} \implies S_{a,0}$ is a linear space (subspace of $H$). 

\begin{center}
    \fbox{\includegraphics[width=10cm, height=4cm]{11.jpg}}
\end{center}

\pagebreak
$S_{a,b} = \{x \in \mathbb{H} | \langle a,x\rangle  = b\} \subset \mathbb{H}$
\\
Let $x_0 \in S_{a,b}$, then $\langle a,x_0\rangle  = b$.
\begin{enumerate}
    \item $\forall x \in S_{a,b}$ \\
    $\langle a, x - x_0\rangle  = \langle a, x\rangle  - \langle a, x_0\rangle  = b - b = 0$ \\
    $\implies x - x_0 \in S_{a,0}$
    $\implies x \in x_0 + S_{a,0} \implies S_{a,b} \subset x_0 + S_{a,0}$
    \item $\forall x \in S_{a,0}$ \\
    $\langle a,x+x_0\rangle  = \langle a,x\rangle  + \langle a,x_0\rangle  = 0 + b = b$ \\
    $\implies x + x_0 \in S_{a,b}$ 
    $\implies S_{a,0} + x_0 \subset S_{a,b}$
\end{enumerate}

(1) and (2) $\implies$ $S_{a,b} = S_{a,0} + x_0$ \\
$S_{a,b}$ is a shift of a subspace.

\begin{center}
    \fbox{\includegraphics[width=10cm, height=4cm]{12.png}}
\end{center}

Thus, $S_{a,b}$ is a plane on $\mathbb{H}$. So we call $S_{a,b}$ a hyperplane. Also, its co-dimension is 1 because it is defined by one linear equation.

\bigbreak
\textbf{Projection Onto Hyperplane} \\
Consider a hyperplane in $\mathbb{H}$
\begin{center}
    $S = \{x \in \mathbb{H} | \langle a,x\rangle  = b\}$
\end{center}
Given $y \in \mathbb{H}$, the vector on $S$ that is closest to y is called the projection of $y$ onto $S$, denoted by $P_Sy$.
\begin{center}
    \item $P_Sy = \underset{x \in S}{\operatorname*{argmin}} \Vert y - x \Vert$ 
\end{center}
Our goal here is to find the explicit form of $P_sy$ in terms of $a$, $b$ and $y$.

\begin{center}
    \fbox{\includegraphics[width=7cm, height=4cm]{13.jpeg}}
\end{center}

\pagebreak

\textbf{Theorem:} $z$ is a solution of $\underset{x \in S}{\operatorname*{argmin}} \Vert y - x \Vert$ $\iff$ $z \in S$ and $\langle z-y,x-z\rangle  = 0$, $\forall x \in S$. 

\bigbreak
\textbf{Remarks:} Since $\forall x \in S$, $x - z \in S - z$ and $z 
\in S \implies x - z \in S_{a,0}$, so $\langle z-y,x-z\rangle  = 0$ implies $z-y \perp S_{a,0}$.

\bigbreak
\textbf{Proof:} ($\Rightarrow$) Assume $z$ is a solution of $\underset{x \in S}{\operatorname*{argmin}} \Vert y - x \Vert$, then $z \in S$. \\
$\forall x \in S$ and $\forall t \in \mathbb{R}$, 
\begin{center}
    $\langle a, z+t(x-z)\rangle  = \langle a,z\rangle  + t(\langle a,x\rangle -\langle a,z\rangle ) = b + t(b-b) = b$
    
\end{center}
Hence, $z + t(x-z) \in S$. \\
Since $z$ is a minimizer,
\begin{center}
    $\Vert z - y \Vert^2 \leq \Vert z + t(x - z) - y\Vert^2 = \Vert z - y + t(x-z) \Vert^2$
    \bigbreak $= \Vert z - y \Vert^2 + 2t\langle z-y,x-z\rangle  + t^2 \Vert x - z\Vert^2$ \bigbreak $\implies$ $2t\langle z-y,x-z\rangle  \ \geq \ -t^2 \Vert x - z \Vert^2$
\end{center}

\begin{itemize}
    \item If $t >  0$, then
    \begin{center}
        $\langle z-y,x-z\rangle  \ \geq \ -\frac{t}{2} \Vert x - z \Vert^2$
    \end{center}
    Let $ t \to 0_+$, then
    \begin{center}
        $\langle z-y,x-z\rangle  \ \geq \ \ \underset{t \to 0_+}{\lim} (-\frac{t}{2} \Vert x - z \Vert^2) = 0$
    \end{center}
    \item If $t < 0$, then
    \begin{center}
        $\langle z-y,x-z\rangle  \ \leq \ -\frac{t}{2} \Vert x - z \Vert^2$
    \end{center}
    Let $ t \to 0_-$, then
    \begin{center}
        $\langle z-y,x-z\rangle  \ \leq \ \underset{t \to 0_-}{\lim}(-\frac{t}{2} \Vert x - z \Vert^2) = 0$
    \end{center}
\end{itemize}
$\implies \langle z-y,x-z\rangle  = 0$ \bigbreak

$(\Leftarrow)$ Assume $z \in S$ and $\langle z-y, x-z\rangle  = 0$, $\forall x \in S$,
\begin{center}
    $\Vert x - y \Vert^2 = \Vert (x - z) + (z - y) \Vert^2$ \bigbreak $= \Vert x - z \Vert^2 + 2 \langle x-z,z-y\rangle  + \Vert z - y \Vert^2 $ \bigbreak $= \Vert x - z \Vert^2 + \Vert z - y \Vert^2 \geq \Vert z - y \Vert^2$ \bigbreak $\implies z = \underset{x \in S}{\operatorname*{argmin}} \Vert x - y \Vert$
\end{center}

\pagebreak

\textbf{Theorem:} Let $\mathbb{H}$ be a Hilbert space and $a \in \mathbb{H}$. Let $b \in \mathbb{R}$ and $S = \{x \in \mathbb{H} | \langle a,x\rangle  = b\}$. Given $y \in \mathbb{H}$, the solution of 
\begin{center}
    $\underset{x \in S}{\operatorname*{argmin}}\Vert y - x \Vert$
\end{center}
exists and is unique, which is given by
\begin{center}
    $y - (\frac{\langle a,y\rangle -b}{\Vert a \Vert^2})a$
\end{center}
    
\bigbreak 

\textbf{Proof:} $z = y - (\frac{\langle a,y\rangle -b}{\Vert a \Vert^2})a$ \bigbreak
Then
\begin{enumerate}
    \item $\langle a,z\rangle  = \langle a,y\rangle  - \langle a, \frac{\langle a,y\rangle -b}{\Vert a \Vert^2} a\rangle $ \bigbreak
          $ = \langle a,y\rangle  - \frac{\langle a,y\rangle -b}{\Vert a \Vert^2} \langle a,a\rangle $ \bigbreak
          $ = \langle a,y\rangle  - (\langle a,y\rangle  - b)$ \bigbreak
          $ = b \implies z \in S$ 
    \item For any $x \in S$, \\
          $\langle z-y,x-z\rangle  = \langle (-\frac{\langle a,y\rangle -b} {\Vert a \Vert^2})a,x-z\rangle $ \bigbreak
          $ = -\frac{\langle a,y\rangle -b}{\Vert a \Vert^2}(\langle a,x\rangle  - \langle a,z\rangle )$ \bigbreak
          $ = - \frac{\langle a,y\rangle -b}{\Vert a \Vert^2}(b - b)$ \bigbreak
          $ = 0$ \\
          
    
\end{enumerate}
Hence, $z$ is a solution of $\underset{x \in S}{\operatorname*{argmin}} \Vert y - x \Vert$. It remains to check the uniqueness. Suppose it has two solutions $z_1$ and $z_2$. Then $z_1$,$z_2\in S$.
\begin{center}
    $z_1$ is a solution $\implies \langle z_1-y,z_2-z_1 \rangle = 0$ \\
    $z_2$ is a solution $\implies \langle z_2-y, z_1-z_2 \rangle = 0 \implies \langle y - z_2, z_2 - z_1 \rangle = 0$ 
\end{center}
Adding the two identities, we have
\begin{center}
    $\langle z_1-z_2,z_2-z_1 \rangle = 0$ \\
    $\iff -\Vert z_1 - z_2 \Vert^2 = 0$ \\
    $\iff z_1 = z_2 $ \textit{(Contradiction)}
\end{center}

\pagebreak

In summary, \\
Let $\mathbb{H}$ be a Hilbert space and 
\begin{center}
    $S = \{x \in \mathbb{H}|\langle a,x \rangle = b\}$
\end{center}
Let $y \in \mathbb{H}$. Then the projection of $y$ onto $S$ is 
\begin{center}
    $P_Sy = \underset{x \in S}{\operatorname*{argmin}} \Vert y - x \Vert$
\end{center}
and can be given by
\begin{center}
    $P_Sy = y - (\frac{\langle a, y \rangle -b}{\Vert a \Vert^2})a$
\end{center}

\textbf{Affine Function} \\
\textbf{Definition:} A linear function plus a constant is an affine function. i.e. $f$ is affine if $f(x) = g(x) + b$ where $g: \mathbb{H} \to \mathbb{R}$ is linear and $b \in \mathbb{R}$. 
\bigbreak
\textbf{Properties:}
\begin{enumerate}
    \item If $f: \mathbb{H} \to \mathbb{R}$ is affine, then for any $\alpha, \beta \in \mathbb{R}$ and $\alpha + \beta = 1$, we have 
    \begin{center}
        $f(\alpha x + \beta y) = \alpha f(x) + \beta f(y)$
    \end{center}
    To see this, 
    \begin{center}
        $f(\alpha x + \beta y) = g(\alpha x + \beta y) + (\alpha + \beta) b$ \bigbreak
        $= \alpha g(x) + \beta g(y) + (\alpha + \beta) b$ \bigbreak
        $= \alpha (g(x) + b) + \beta (g(y) + b)$ \bigbreak
        $= \alpha f(x) + \beta f(y)$
    \end{center}
    \item If $\mathbb{H}$ is a Hilbert space and $f$ is bounded, then $f$ is affine if and only if 
    \begin{center}
        $f(x) = \langle a, x \rangle + b$ for some $a \in \mathbb{H}$ and $b \in \mathbb{R}$
    \end{center}
\end{enumerate}
\end{document}

